# Standard library imports
import os
import json
import time
import glob
from datetime import timedelta
from abc import ABC, abstractmethod
from typing import Literal, Annotated

# Third-party imports
import torch
import dotenv
from tqdm import tqdm
from openai import OpenAI
from omegaconf import OmegaConf
import torch.distributed as dist
from datasets import load_dataset
from pydantic import ValidationError
from torch.nn.parallel import DistributedDataParallel as DDP
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# Local imports
from src.utils.log.manager import Logger
from src.utils.type.schema import RootConfig, PromptConfig, TeacherFactoryParams


class GenerationStrategy(ABC):
    @abstractmethod
    def generate(self, context):
        pass


class SingleGenerationStrategy(GenerationStrategy):
    def generate(self, context):
        # noinspection PyProtectedMember
        return context._generate_single()


class BatchGenerationStrategy(GenerationStrategy):
    def generate(self, context):
        # noinspection PyProtectedMember
        return context._generate_batch()


class DDPGenerationStrategy(GenerationStrategy):
    def generate(self, context):
        # noinspection PyProtectedMember
        return context._generate_distributed()


class Teacher(ABC):
    """
    Abstract base class representing a Teacher for instruction generation tasks.

    This class defines a configurable interface for models responsible for
    generating prompts or responses, typically used in training student models.

    Parameters
    ----------
    config : RootConfig
        Configuration object containing teacher parameters and paths.
    dataset_path : str
        File path to the dataset used by the teacher.

    Attributes
    ----------
    config : RootConfig
        Configuration object for accessing teacher settings.
    dataset_path : str
        Path to the dataset file.
    prompt_length : int
        Maximum length of the prompt.
    output_path : str
        Output path where generated data may be saved.
    generation_strategy : GenerationStrategy or None
        Strategy object used to control how content is generated.

    """

    def __init__(
            self,
            config: Annotated[RootConfig, "Configuration for the teacher model"],
            dataset_path: Annotated[str, "Path to the training dataset"]
    ):
        if not isinstance(dataset_path, str):
            raise TypeError("Expected str for parameter dataset_path")

        self.config = config
        self.dataset_path = dataset_path

        base_conf = self.config.teacher.base
        self.prompt_length = base_conf.prompt_length
        self.output_path = base_conf.output_path

        self.generation_strategy = None

    def set_generate_strategy(
            self,
            generation_strategy: Annotated[
                GenerationStrategy, "Strategy for generating content"
            ]
    ) -> None:
        """
        Set the generation strategy for the teacher.

        Parameters
        ----------
        generation_strategy : GenerationStrategy
            An object implementing the generation interface.

        Returns
        -------
        None

        """
        if not isinstance(generation_strategy, GenerationStrategy):
            raise TypeError("Expected GenerationStrategy for parameter generation_strategy")
        self.generation_strategy = generation_strategy

    @abstractmethod
    def _prompt(
            self,
            instruction: Annotated[str, "Instruction to be converted to a prompt"]
    ) -> Annotated[str, "Prompt string generated from instruction"]:
        """
        Abstract method to construct a prompt from a given instruction.

        Parameters
        ----------
        instruction : str
            Instruction string to generate a prompt for.

        Returns
        -------
        str
            Generated prompt.
        """
        pass

    def generate(self) -> Annotated[str, "Generated output from the strategy"]:
        """
        Generate output using the configured generation strategy.

        Returns
        -------
        str
            Output string generated by the strategy.

        Raises
        ------
        ValueError
            If no generation strategy is set.

        """
        if self.generation_strategy is None:
            raise ValueError("Generation strategy not set!")
        return self.generation_strategy.generate(self)


class TeacherFactory:
    """
    Factory class to create and configure Teacher instances.

    Based on the specified `model_type`, this factory creates either an
    open-source teacher model (`TeacherOpen`) or a closed (API-based) teacher model
    (`TeacherClosed`). It also supports downloading appropriate datasets.

    Parameters
    ----------
    config : RootConfig
        Configuration object for model and dataset settings.
    dataset_path : str
        Path to the instruction dataset to be used.
    model_type : {'open', 'closed'}, optional
        Type of teacher model to use. Default is 'open'.

    Attributes
    ----------
    config : RootConfig
        Loaded configuration settings.
    dataset_path : str
        Path to the dataset file.
    model_type : str
        Validated model type ('open' or 'closed').
    logger : logging.Logger
        Logger instance for status and progress reporting.

    Examples
    --------
    >>> from src.utils.type.schema import RootConfig
    >>> from omegaconf import OmegaConf
    >>> test_config_path = "src/config/config.yaml"
    >>> test_conf_dict = OmegaConf.to_container(OmegaConf.load(test_config_path), resolve=True)
    >>> config = RootConfig(**conf_dict)
    >>> factory = TeacherFactory(config, "data/instructions.jsonl", model_type="open")
    >>> teacher = factory.build()
    >>> factory.download()
    """

    def __init__(
            self,
            config: Annotated[RootConfig, "Configuration for the teacher model"],
            dataset_path: Annotated[str, "Path to the dataset file"],
            model_type: Annotated[Literal["open", "closed"], "Model type: 'open' or 'closed'"] = "open"
    ):
        if not isinstance(dataset_path, str):
            raise TypeError("Expected str for parameter dataset_path")
        if model_type not in ("open", "closed"):
            raise ValueError("model_type must be either 'open' or 'closed'")

        self.config = config
        self.dataset_path = dataset_path

        validated = TeacherFactoryParams(model_type=model_type)
        self.model_type = validated.model_type

        self.logger = Logger(name="TeacherFactory").get()

    def build(self):
        """
        Build and return a teacher instance based on the model type.

        Returns
        -------
        Teacher
            A `TeacherOpen` or `TeacherClosed` instance based on configuration.

        Raises
        ------
        ValueError
            If the model type is unsupported.

        Examples
        --------
        >>> teacher = TeacherFactory.build()
        """
        if self.model_type == "open":
            return TeacherOpen(self.config, self.dataset_path)
        elif self.model_type == "closed":
            return TeacherClosed(self.config, self.dataset_path)
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")

    def download(self):
        """
        Download the dataset for the specified model type from the Hugging Face Hub.

        The dataset revision is selected based on the configured download size.

        Returns
        -------
        None

        Raises
        ------
        ValueError
            If the model type or download size is invalid.

        Examples
        --------
        >>> TeacherFactory.download()
        """
        if self.model_type == "open":
            download_size = self.config.teacher.open.data.download.size
            output_path = self.config.teacher.base.output_path
            size_revision_map = {
                "16k": "v3-GoogleDocString",
                "32k": "v5-GoogleDocString",
                "64k": "v7-GoogleDocString"
            }
            dataset_name = "bunyaminergen/Stable-Code-Python-SFT"

        elif self.model_type == "closed":
            download_size = self.config.teacher.closed.data.download.size
            output_path = self.config.teacher.base.output_path
            size_revision_map = {
                "16k": "v3-NumpyDocString",
                "32k": "v5-NumpyDocString",
                "64k": "v7-NumpyDocString"
            }
            dataset_name = "bunyaminergen/Stable-Code-Python-SFT"

        else:
            raise ValueError(f"Unknown model type: {self.model_type}")

        if os.path.exists(output_path):
            self.logger.info(f"{output_path} already exists. Skipping download.")
            return

        revision = size_revision_map.get(download_size)
        if not revision:
            raise ValueError(f"Invalid download size: {download_size}")

        self.logger.info(
            f"Downloading {dataset_name} dataset from HF with size='{download_size}' => revision='{revision}'."
        )
        dataset = load_dataset(dataset_name, revision=revision, split="train")

        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        dataset.to_json(output_path, orient="records", lines=True)
        self.logger.info(f"Dataset downloaded => {output_path}")


class TeacherOpen(Teacher):
    """
    TeacherOpen is a subclass of Teacher that loads and interacts with
    open-source language models for generating responses.

    This class handles the initialization of the tokenizer and model using
    Hugging Face Transformers, applies optional optimizations, and configures
    generation and tokenization parameters.

    Parameters
    ----------
    config : RootConfig
        Configuration object with nested model, pipeline, tokenizer, and data
        settings.
    dataset_path : str
        Path to the dataset file for instruction inputs.

    Attributes
    ----------
    model : AutoModelForCausalLM
        The initialized causal language model.
    tokenizer : AutoTokenizer
        The tokenizer corresponding to the model.
    model_name : str
        Name of the model to load.
    temperature : float
        Sampling temperature for generation.
    batch_size : int
        Number of inputs per generation batch.
    top_p : float
        Top-p sampling for nucleus filtering.
    ...
    """

    def __init__(
            self,
            config: Annotated[RootConfig, "Configuration object for TeacherOpen"],
            dataset_path: Annotated[str, "Path to the dataset"]
    ):
        super().__init__(config, dataset_path)

        log_manager = Logger(name="TeacherOpen")
        self.logger = log_manager.get()

        # CUDA Configuration
        cuda = self.config.cuda
        device_type = self.config.device.type

        # Teacher configuration
        teacher_conf = self.config.teacher.open
        model_conf = teacher_conf.model
        pipeline_conf = teacher_conf.pipeline
        tokenizer_conf = teacher_conf.tokenizer
        data_conf = teacher_conf.data

        # Model settings
        self.model_name = model_conf.name
        self.model_save_path = model_conf.save_path
        self.trust_remote_code = model_conf.trust_remote_code
        self.attn_implementation = model_conf.attn_implementation
        self.bfloat16 = model_conf.bfloat16

        # Pipeline (generation) settings
        self.max_new_tokens = pipeline_conf.max_new_tokens
        self.temperature = pipeline_conf.temperature
        self.top_p = pipeline_conf.top_p
        self.top_k = pipeline_conf.top_k
        self.use_cache = pipeline_conf.use_cache
        self.do_sample = pipeline_conf.do_sample
        self.batch_size = pipeline_conf.batch_size

        # Tokenizer settings
        self.additional_special_tokens = tokenizer_conf.additional_special_tokens
        self.padding_side = tokenizer_conf.padding_side
        self.add_generation_prompt = tokenizer_conf.add_generation_prompt
        self.tokenize = tokenizer_conf.tokenize
        self.padding = tokenizer_conf.padding
        self.truncation = tokenizer_conf.truncation
        self.skip_special_tokens = tokenizer_conf.skip_special_tokens
        self.clean_up_tokenization_spaces = tokenizer_conf.clean_up_tokenization_spaces
        self.use_fast = tokenizer_conf.use_fast

        # Data settings
        self.download_size = data_conf.download.size

        # GPU / TF32 optimization
        if torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_tf32 = cuda.allow_tf32
            self.logger.info("TF32 enabled for faster matmul on supported GPUs.")

        # Tokenizer initialization
        self.logger.info(f"Loading tokenizer from: {self.model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            cache_dir=self.model_save_path,
            trust_remote_code=self.trust_remote_code,
            padding_side=self.padding_side,
            use_fast=self.use_fast,
        )
        self.tokenizer.add_special_tokens({
            "additional_special_tokens": self.additional_special_tokens
        })
        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # Model initialization
        self.logger.info(f"Loading model from: {self.model_name}")
        torch_dtype = torch.bfloat16 if self.bfloat16 else torch.float16
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            cache_dir=self.model_save_path,
            trust_remote_code=self.trust_remote_code,
            attn_implementation=self.attn_implementation,
            torch_dtype=torch_dtype,
            device_map=device_type,
        )
        self.model.resize_token_embeddings(len(self.tokenizer))

        try:
            self.model = torch.compile(self.model)
            self.logger.info("Model compiled with torch.compile for potential speedup.")
        except Exception as e:
            self.logger.warning(f"torch.compile failed or not supported: {e}")

        self.model.eval()

    def _prompt(
            self,
            instruction: Annotated[str, "Instruction to format with system prompt"]
    ) -> Annotated[str, "Formatted prompt for use with the model"]:
        """
        Format the input instruction using a system prompt.

        Loads a system prompt from configuration and structures the message for
        the assistant to process the user's input.

        Parameters
        ----------
        instruction : str
            The instruction input from the dataset.

        Returns
        -------
        str
            Formatted prompt string with roles for system and user.

        Raises
        ------
        ValueError
            If the prompt configuration file is missing or invalid.
        """
        if not isinstance(instruction, str):
            raise TypeError("Expected str for parameter instruction")

        try:
            cfg = OmegaConf.load("src/config/prompt.yaml")
            raw_data = OmegaConf.to_container(cfg, resolve=True)
            validated_prompts = PromptConfig(**raw_data)
            teacher_system_prompt = validated_prompts.Teacher.system
        except (FileNotFoundError, ValidationError) as e:
            raise ValueError(f"Problem with prompt.yaml config: {e}")

        messages = [
            {"role": "system", "content": teacher_system_prompt},
            {"role": "user", "content": instruction},
        ]

        return (
            f"System: {messages[0]['content']}\n"
            f"User: {messages[1]['content']}\n"
            "Assistant:"
        )

    def _generate_single(self) -> Annotated[str, "Path to the output file"]:
        """
        Generate responses using a single GPU or CPU in batch mode.

        This method loads the dataset, tokenizes the prompts, performs generation
        using the model, and appends the results to a JSONL output file. It also
        supports resuming if the file already contains some results.

        Returns
        -------
        str
            Path to the output file containing generated responses.

        Raises
        ------
        OSError
            If directories or output files cannot be created.

        """
        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)

        processed_instructions = 0
        if os.path.exists(self.output_path):
            with open(self.output_path, encoding="utf-8") as f:
                processed_instructions = sum(1 for _ in f)
            self.logger.info(
                f"{self.output_path} already exists => "
                f"{processed_instructions} lines. Will resume from there."
            )

        dataset = load_dataset("json", data_files=self.dataset_path, split="train")
        instructions = dataset["instruction"]
        total_instructions = len(instructions)
        self.logger.info(
            f"There are {total_instructions} instructions to process. (single mode)"
        )

        model_for_generate = self.model

        with open(self.output_path, "a", encoding="utf-8") as f_out:
            with torch.inference_mode():
                for i in tqdm(
                        range(processed_instructions, total_instructions, self.batch_size),
                        desc="Generating"
                ):
                    batch_instructions = instructions[i: i + self.batch_size]
                    batch_prompts = [self._prompt(instr) for instr in batch_instructions]

                    inputs = self.tokenizer(
                        batch_prompts,
                        return_tensors="pt",
                        padding=self.padding,
                        truncation=self.truncation
                    ).to(model_for_generate.device)

                    tokens = model_for_generate.generate(
                        **inputs,
                        max_new_tokens=self.max_new_tokens,
                        temperature=self.temperature,
                        top_p=self.top_p,
                        top_k=self.top_k,
                        do_sample=self.do_sample,
                        use_cache=self.use_cache
                    )

                    for idx in range(len(batch_prompts)):
                        start_idx = inputs.input_ids.shape[-1]
                        out_str = self.tokenizer.decode(
                            tokens[idx, start_idx:],
                            skip_special_tokens=self.skip_special_tokens,
                            clean_up_tokenization_spaces=self.clean_up_tokenization_spaces
                        )

                        record = {
                            "instruction": batch_instructions[idx],
                            "output": out_str
                        }
                        f_out.write(json.dumps(record, ensure_ascii=False) + "\n")

        self.logger.info(f"Single-GPU (or CPU) generation finished => {self.output_path}")
        return self.output_path

    def _generate_distributed(self) -> Annotated[str, "Path to the merged output file"]:
        """
        Perform distributed generation using PyTorch's DistributedDataParallel (DDP).

        Each rank handles a shard of the instruction dataset, generates responses,
        writes them to a rank-specific file, and rank 0 merges them at the end.

        Returns
        -------
        str
            Path to the merged output file after distributed generation.

        Raises
        ------
        ValueError
            If local rank exceeds the number of available GPUs.

        """
        ddp_is_enabled = ("LOCAL_RANK" in os.environ) and ("RANK" in os.environ)
        if not ddp_is_enabled:
            self.logger.warning(
                "[DDP] Required environment variables not found. "
                "Falling back to single GPU."
            )
            return self._generate_single()

        if not dist.is_available():
            self.logger.warning("[DDP] torch.distributed is not available. Using single GPU.")
            return self._generate_single()

        if not dist.is_initialized():
            local_rank = int(os.environ["LOCAL_RANK"])
            world_size = int(os.environ.get("WORLD_SIZE", "1"))
            ngpu = torch.cuda.device_count()
            if local_rank >= ngpu:
                raise ValueError(f"[DDP] local_rank={local_rank} but only {ngpu} GPU(s) available.")

            backend = "nccl" if world_size > 1 else "gloo"

            dist.init_process_group(
                backend=backend,
                init_method="env://",
                timeout=timedelta(hours=24)
            )
            self.logger.info(
                f"[DDP] Initialized process group. "
                f"rank={dist.get_rank()}, world_size={dist.get_world_size()}"
            )
            torch.cuda.set_device(local_rank)
            ddp_is_freshly_inited = True
        else:
            self.logger.info("[DDP] Process group already initialized.")
            local_rank = int(os.environ.get("LOCAL_RANK", 0))
            ddp_is_freshly_inited = False

        if not isinstance(self.model, DDP):
            self.model = DDP(
                self.model,  # type: ignore
                device_ids=[local_rank],
                output_device=local_rank
            )

        dist.barrier()
        rank = dist.get_rank()

        dataset = load_dataset("json", data_files=self.dataset_path, split="train")
        instructions = dataset["instruction"]
        total_instructions = len(instructions)

        chunk_size = (total_instructions + dist.get_world_size() - 1) // dist.get_world_size()
        start_index = rank * chunk_size
        end_index = min(start_index + chunk_size, total_instructions)
        local_instructions = instructions[start_index:end_index]

        shard_path = self.output_path.replace(".jsonl", f"_rank_{rank}.jsonl")
        os.makedirs(os.path.dirname(shard_path), exist_ok=True)
        self.logger.info(f"Rank {rank} => Shard path: {shard_path}")

        processed_instructions = 0
        if os.path.exists(shard_path):
            with open(shard_path, encoding="utf-8") as f:
                processed_instructions = sum(1 for _ in f)
            self.logger.info(f"Rank {rank} => Resuming from {processed_instructions} lines.")

        model_for_generate = self.model.module
        model_for_generate.eval()

        with open(shard_path, "a", encoding="utf-8") as f_out:
            with torch.inference_mode():
                for i in tqdm(
                        range(processed_instructions, len(local_instructions), self.batch_size),
                        desc=f"Rank={rank}"
                ):
                    batch_instructions = local_instructions[i:i + self.batch_size]
                    batch_prompts = [self._prompt(instr) for instr in batch_instructions]

                    inputs = self.tokenizer(
                        batch_prompts,
                        return_tensors="pt",
                        padding=self.padding,
                        truncation=self.truncation
                    ).to(torch.device(f"cuda:{local_rank}"))

                    tokens = model_for_generate.generate(
                        **inputs,
                        max_new_tokens=self.max_new_tokens,
                        temperature=self.temperature,
                        top_p=self.top_p,
                        top_k=self.top_k,
                        do_sample=self.do_sample,
                        use_cache=self.use_cache
                    )

                    for idx, instr in enumerate(batch_instructions):
                        start_idx = inputs.input_ids.shape[-1]
                        out_str = self.tokenizer.decode(
                            tokens[idx, start_idx:],
                            skip_special_tokens=self.skip_special_tokens,
                            clean_up_tokenization_spaces=self.clean_up_tokenization_spaces
                        )
                        record = {"instruction": instr, "output": out_str}
                        f_out.write(json.dumps(record, ensure_ascii=False) + "\n")

        dist.barrier()

        if rank == 0:
            self.logger.info("[DDP] All ranks completed. Merging shards.")
            pattern = self.output_path.replace(".jsonl", "") + "_rank_*.jsonl"
            shard_files = sorted(glob.glob(pattern))
            self.logger.info(f"Found shard files => {shard_files}")

            with open(self.output_path, "w", encoding="utf-8") as fout:
                for sf in shard_files:
                    with open(sf, encoding="utf-8") as fin:
                        for line in fin:
                            fout.write(line)

            for sf in shard_files:
                os.remove(sf)
            self.logger.info(f"[DDP] Merge completed => {self.output_path}")

        dist.barrier()

        if ddp_is_freshly_inited and dist.is_initialized():
            dist.destroy_process_group()
            self.logger.info("[DDP] Process group destroyed.")

        return self.output_path

    def _generate_batch(self) -> Annotated[str, "Raises NotImplementedError"]:
        """
        Not implemented for TeacherOpen, as batch mode is only supported in closed-source APIs.

        Returns
        -------
        str
            This function does not return. Always raises an error.

        Raises
        ------
        NotImplementedError
            Always raised because batch mode is unsupported for TeacherOpen.
        """
        raise NotImplementedError("Batch mode is not supported for TeacherOpen")


class TeacherClosed(Teacher):
    """
    TeacherClosed is a specialized subclass of Teacher that uses OpenAI's
    closed-source API to generate responses to instructions from a dataset.

    This class handles configuration loading, API client initialization, and
    batch response generation with logging and error handling.

    Parameters
    ----------
    config : RootConfig
        The root configuration object containing all necessary parameters.
    dataset_path : str
        Path to the dataset containing instructions to be processed.

    Attributes
    ----------
    model_name : str
        Name of the model used by the OpenAI client.
    temperature : float
        Temperature parameter for sampling in generation.
    batch_size : int
        Number of instructions processed in one batch.
    top_p : float
        Top-p nucleus sampling parameter.
    download_size : int
        Size of the dataset download.
    client : OpenAI
        OpenAI client for calling the API.
    logger : logging.Logger
        Logger instance for debug/info/error messages.
    """

    def __init__(
            self,
            config: Annotated[RootConfig, "The root configuration for the teacher"],
            dataset_path: Annotated[str, "Path to the dataset with instructions"]
    ):
        super().__init__(config, dataset_path)
        self.logger = Logger(name="TeacherClosed").get()

        # Teacher Closed Configurations
        closed_conf = self.config.teacher.closed
        model_conf = closed_conf.model
        pipeline_conf = closed_conf.pipeline
        data_conf = closed_conf.data

        self.model_name = model_conf.name
        self.temperature = pipeline_conf.temperature
        self.batch_size = pipeline_conf.batch_size
        self.top_p = pipeline_conf.top_p
        self.download_size = data_conf.download.size

        # Credentials
        env_variables = dotenv.dotenv_values()
        openai_api_key = env_variables.get("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables.")

        # OpenAI Client
        self.client = OpenAI(api_key=openai_api_key)

    def _prompt(
            self,
            instruction: Annotated[str, "User instruction to format with system prompt"]
    ) -> Annotated[str, "Formatted prompt for OpenAI API"]:
        """
        Format a prompt using system and user instruction.

        Parameters
        ----------
        instruction : str
            The user instruction text.

        Returns
        -------
        str
            Formatted prompt string.

        Raises
        ------
        ValueError
            If prompt configuration file is missing or invalid.

        """

        try:
            cfg = OmegaConf.load("src/config/prompt.yaml")
            raw_data = OmegaConf.to_container(cfg, resolve=True)
            validated_prompts = PromptConfig(**raw_data)
            teacher_system_prompt = validated_prompts.Teacher.system
        except (FileNotFoundError, ValidationError) as e:
            raise ValueError(f"Problem with prompt.yaml config: {e}")

        return (
            f"System: {teacher_system_prompt}\n"
            f"User: {instruction}\n"
            "Assistant:"
        )

    def _generate_single(self) -> Annotated[str, "Path to the output file"]:
        """
        Generate responses for each instruction in the dataset using OpenAI API.

        Returns
        -------
        str
            Path to the output file containing generated records.

        Raises
        ------
        OSError
            If the dataset path is invalid or output directory cannot be created.

        """
        output_dir = os.path.dirname(self.output_path)
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)

        dataset = load_dataset("json", data_files=self.dataset_path, split="train")
        instructions = dataset["instruction"]
        total_instructions = len(instructions)
        self.logger.info(f"Total instructions: {total_instructions} (Closed/OpenAI)")

        processed_instructions = 0
        if os.path.exists(self.output_path):
            with open(self.output_path, encoding="utf-8") as f:
                processed_instructions = sum(1 for _ in f)
            self.logger.info(f"Resuming from line={processed_instructions}")

        with open(self.output_path, "a", encoding="utf-8") as f_out:
            for i in tqdm(
                    range(processed_instructions, total_instructions, self.batch_size),
                    desc="GeneratingClosed"
            ):
                batch_instructions = instructions[i: i + self.batch_size]
                for instruction in batch_instructions:
                    try:
                        response = self.client.responses.create(
                            model=self.model_name,
                            instructions="You are a Python Developer.",
                            input=instruction,
                            temperature=self.temperature,
                            top_p=self.top_p,
                        )
                        output_str = response.output_text
                    except Exception as e:
                        output_str = f"[OpenAI API Error] => {e}"
                        self.logger.error(output_str)

                    record = {"instruction": instruction, "output": output_str}
                    f_out.write(json.dumps(record, ensure_ascii=False) + "\n")

        self.logger.info(f"Finished synchronous generate => {self.output_path}")
        return self.output_path

    def _generate_batch(self) -> Annotated[str, "Path to the output file"]:
        """
        Generate responses for a batch of instructions using OpenAI's batch API.

        This method sends a batch of instructions to the OpenAI API, handles
        the response, logs progress, and writes the output to a file.

        Returns
        -------
        str
            The path to the output file where responses are stored.

        Raises
        ------
        OSError
            If directories or files cannot be created or accessed.
        ValueError
            If API response or content is invalid.

        """
        batch_input_dir = ".data/dataset/teacher"
        os.makedirs(batch_input_dir, exist_ok=True)
        local_batch_input_file = os.path.join(batch_input_dir, "batch_input.jsonl")

        custom_id_map = {}

        try:
            dataset = load_dataset("json", data_files=self.dataset_path, split="train")
            instructions = dataset["instruction"]
            total_instructions = len(instructions)
            self.logger.info(f"Batch Generation: total_instructions={total_instructions}")

            with open(local_batch_input_file, "w", encoding="utf-8") as f:
                for idx, instr in enumerate(instructions):
                    custom_id = f"request-{idx}"
                    custom_id_map[custom_id] = instr

                    line_data = {
                        "custom_id": custom_id,
                        "method": "POST",
                        "url": "/v1/chat/completions",
                        "body": {
                            "model": self.model_name,
                            "messages": [
                                {"role": "developer", "content": "You are a Python Developer."},
                                {"role": "user", "content": instr},
                            ],
                            "temperature": self.temperature,
                            "top_p": self.top_p
                        }
                    }
                    f.write(json.dumps(line_data) + "\n")

            self.logger.info("Local batch input file created => %s", local_batch_input_file)

            try:
                with open(local_batch_input_file, "rb") as fin:
                    batch_input_file = self.client.files.create(file=fin, purpose="batch")
                batch_input_file_id = batch_input_file.id
                self.logger.info(f"Uploaded batch input file => {batch_input_file_id}")
            except Exception as e:
                self.logger.error(f"Error uploading batch file => {e}")
                return self.output_path

            try:
                new_batch = self.client.batches.create(
                    input_file_id=batch_input_file_id,
                    endpoint="/v1/chat/completions",
                    completion_window="24h",
                    metadata={"description": "TeacherClosed nightly job"}
                )
                batch_id = new_batch.id
                self.logger.info(f"Batch created => {batch_id}, status={new_batch.status}")
            except Exception as e:
                self.logger.error(f"Error creating batch => {e}")
                return self.output_path

            while True:
                time.sleep(60)
                try:
                    current_batch = self.client.batches.retrieve(batch_id)
                except Exception as e:
                    self.logger.error(f"Error retrieving batch => {e}")
                    return self.output_path

                status = current_batch.status
                self.logger.info(f"Batch {batch_id} => status={status}")

                if status in ["validating", "in_progress", "finalizing"]:
                    continue
                elif status == "completed":
                    self.logger.info("Batch COMPLETED!")
                    break
                elif status == "failed":
                    self.logger.error("Batch FAILED during validation.")
                    return self.output_path
                elif status == "expired":
                    self.logger.error("Batch EXPIRED (not completed in 24h).")
                    return self.output_path
                elif status == "cancelled":
                    self.logger.error("Batch CANCELLED manually.")
                    return self.output_path
                else:
                    self.logger.warning(f"Batch ended with unknown status: {status}")
                    return self.output_path

            output_file_id = current_batch.output_file_id
            error_file_id = current_batch.error_file_id
            self.logger.info(f"output_file_id={output_file_id}, error_file_id={error_file_id}")

            if not output_file_id:
                self.logger.warning("No output_file_id => Possibly no successful requests.")
                return self.output_path

            try:
                file_response = self.client.files.content(output_file_id)
                lines = file_response.text.strip().split("\n")
            except Exception as e:
                self.logger.error(f"Error downloading batch output => {e}")
                return self.output_path

            self.logger.info(f"Appending batch results => {self.output_path}")
            os.makedirs(os.path.dirname(self.output_path), exist_ok=True)

            if os.path.exists(self.output_path):
                with open(self.output_path, encoding="utf-8") as f:
                    processed_instructions = sum(1 for _ in f)
                self.logger.info(
                    f"{self.output_path} already has {processed_instructions} lines => appending new lines."
                )

            with open(self.output_path, "a", encoding="utf-8") as f_out:
                for line in lines:
                    try:
                        obj = json.loads(line)
                    except json.JSONDecodeError as e:
                        self.logger.error(f"JSON parse error on batch output line => {e}")
                        continue

                    custom_id = obj.get("custom_id", "")
                    response_data = obj.get("response", {})
                    error_data = obj.get("error", None)

                    original_instr = custom_id_map.get(custom_id, f"[Batch custom_id={custom_id}]")

                    if error_data:
                        record = {
                            "instruction": original_instr,
                            "output": f"[ERROR] => {error_data}"
                        }
                    else:
                        body = response_data.get("body", {})
                        choices = body.get("choices", [])
                        if choices:
                            content = choices[0]["message"]["content"]
                            record = {"instruction": original_instr, "output": content}
                        else:
                            record = {"instruction": original_instr, "output": "[No content]"}

                    f_out.write(json.dumps(record, ensure_ascii=False) + "\n")

            self.logger.info(f"Batch results appended => {self.output_path}")
            return self.output_path

        finally:
            if os.path.exists(local_batch_input_file):
                os.remove(local_batch_input_file)
                self.logger.info(f"Removed temporary batch file => {local_batch_input_file}")

    def _generate_distributed(self) -> Annotated[str, "Raises an error if called"]:
        """
        Raise an error for unsupported distributed generation mode.

        Returns
        -------
        str
            Always raises NotImplementedError.

        Raises
        ------
        NotImplementedError
            Always raised to indicate this mode is unsupported.

        """
        raise NotImplementedError("Distributed mode is not supported for TeacherClosed.")


class Assistant:
    """
    Assistant class for managing and using a transformer-based assistant model.

    This class handles loading a base or fine-tuned model from a local directory
    or Hugging Face Hub, configuring tokenizer and quantization options, and
    running inference.

    Parameters
    ----------
    config : RootConfig
        Configuration object containing model setup and training options.

    Attributes
    ----------
    assistant_save_path : str
        Path to save/load the assistant model.
    finetuned_repo_id : str
        HF repo ID for fine-tuned model fallback.
    base_model_name : str
        Name of the base model.
    base_model_save_path : str
        Path to save/load the base model.
    padding_side : str
        Tokenizer padding side configuration.
    skip_special_tokens : bool
        Whether to skip special tokens during decoding.
    max_new_tokens : int
        Maximum number of tokens to generate during inference.
    load_in_4bit : bool
        Whether to load the model in 4-bit precision.
    bfloat16 : bool
        Whether to use bfloat16 precision.
    bnb_4bit_use_double_quant : bool
        Whether to use double quantization.
    bnb_4bit_quant_type : str
        Type of quantization to apply.
    attn_implementation : str
        Attention mechanism implementation used by the model.
    model : AutoModelForCausalLM or None
        The loaded transformer model.
    tokenizer : AutoTokenizer or None
        The tokenizer used with the model.

    Examples
    --------
    >>> from src.model.core import Assistant
    >>> from src.utils.type.schema import RootConfig
    >>> test_config_path = "src/config/config.yaml"
    >>> test_config_load = OmegaConf.load(config_path)
    >>> test_conf_dict = OmegaConf.to_container(config_load, resolve=True)
    >>> config = RootConfig(**conf_dict)
    >>> assistant = Assistant(config)
    >>> assistant.load()
    >>> print(assistant.inference("What is a binary tree?"))
    """

    def __init__(self, config: Annotated[RootConfig, "Configuration object"]):
        self.config = config
        self.assistant_save_path = config.train.sft.save_path
        self.finetuned_repo_id = "bunyaminergen/Qwen2.5-Coder-1.5B-Instruct-SFT"
        self.base_model_name = config.assistant.model.name
        self.base_model_save_path = config.assistant.model.save_path

        self.padding_side = config.assistant.tokenizer.padding_side
        self.skip_special_tokens = config.assistant.tokenizer.skip_special_tokens
        self.max_new_tokens = config.assistant.inference.max_new_tokens

        quant_cfg = config.assistant.quantization
        self.load_in_4bit = quant_cfg.load_in_4bit
        self.bfloat16 = quant_cfg.bfloat16
        self.bnb_4bit_use_double_quant = quant_cfg.bnb_4bit_use_double_quant
        self.bnb_4bit_quant_type = quant_cfg.bnb_4bit_quant_type

        self.attn_implementation = config.assistant.model.attn_implementation

        self.model = None
        self.tokenizer = None

    def load(self, model: Annotated[str, "Either 'base' or 'assistant'"] = "base") -> None:
        """
        Load the base or fine-tuned assistant model and tokenizer.

        Parameters
        ----------
        model : str, optional
            Which model to load: 'base' or 'assistant'. Defaults to 'base'.

        Returns
        -------
        None

        Raises
        ------
        ValueError
            If loading fails due to configuration or download issues.

        Examples
        --------
        >>> from src.model.core import Assistant
        >>> from src.utils.type.schema import RootConfig
        >>> test_config_path = "src/config/config.yaml"
        >>> test_config_load = OmegaConf.load(config_path)
        >>> test_conf_dict = OmegaConf.to_container(config_load, resolve=True)
        >>> config = RootConfig(**conf_dict)
        >>> assistant = Assistant(config)
        >>> response = assistant.inference("Explain recursion.")
        >>> print(response)
        >>> assistant.load("assistant")
        """
        if not isinstance(model, str):
            raise TypeError("Expected str for parameter model")

        model_key = model.lower().strip()
        if model_key == "assistant":
            repo_id = self.finetuned_repo_id
            local_path = self.assistant_save_path
        else:
            repo_id = self.base_model_name
            local_path = self.base_model_save_path

        os.makedirs(local_path, exist_ok=True)
        print(f"Loading tokenizer from {repo_id} into {local_path}...")

        if not os.listdir(local_path):
            print(f"Cache empty: downloading {repo_id}...")
            self.tokenizer = AutoTokenizer.from_pretrained(repo_id, cache_dir=local_path)
            self._fix_tokenizer()
            quant_config = self._get_quant_config()
            self.model = AutoModelForCausalLM.from_pretrained(
                repo_id,
                cache_dir=local_path,
                attn_implementation=self.attn_implementation,
                torch_dtype=torch.bfloat16 if self.bfloat16 else torch.float16,
                quantization_config=quant_config,
                device_map=self.config.device.type,
            )
            self.model.save_pretrained(local_path)
            self.tokenizer.save_pretrained(local_path)
            print(f"Model & tokenizer saved to {local_path}.")
        else:
            flat_path = self._flatten_cache(local_path)
            print(f"Found cache folder: {flat_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(flat_path)
            self._fix_tokenizer()
            quant_config = self._get_quant_config()
            self.model = AutoModelForCausalLM.from_pretrained(
                flat_path,
                attn_implementation=self.attn_implementation,
                torch_dtype=torch.bfloat16 if self.bfloat16 else torch.float16,
                quantization_config=quant_config,
                device_map=self.config.device.type,
            )

        self.model.eval()
        print(f"{model_key.capitalize()} model loaded successfully.")

    @staticmethod
    def _flatten_cache(path: Annotated[str, "Base path to scan for HF snapshot"]) -> Annotated[
        str, "Resolved snapshot path"]:
        """
        Locate and return the snapshot directory within a cache path.

        Parameters
        ----------
        path : str
            The root cache directory to search.

        Returns
        -------
        str
            The resolved path to the model snapshot directory.
        """
        if os.path.exists(os.path.join(path, "tokenizer.json")):
            return path

        try:
            entries = os.listdir(path)
        except OSError:
            return path

        hf_dirs = [d for d in entries if d.startswith("models--")]
        if not hf_dirs:
            return path

        snap_root = os.path.join(path, hf_dirs[0], "snapshots")
        try:
            snaps = [d for d in os.listdir(snap_root) if os.path.isdir(os.path.join(snap_root, d))]
        except OSError:
            return path

        return os.path.join(snap_root, snaps[0]) if snaps else path

    def _fix_tokenizer(self) -> None:
        """
        Ensure tokenizer padding token is set correctly.

        Returns
        -------
        None
        """
        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        self.tokenizer.padding_side = self.padding_side

    def _get_quant_config(self) -> Annotated[BitsAndBytesConfig, "Quantization config object"]:
        """
        Construct the quantization configuration for BitsAndBytes.

        Returns
        -------
        BitsAndBytesConfig
            Configuration for quantized model loading.
        """
        return BitsAndBytesConfig(
            load_in_4bit=self.load_in_4bit,
            bnb_4bit_compute_dtype=torch.bfloat16 if self.bfloat16 else torch.float16,
            bnb_4bit_use_double_quant=self.bnb_4bit_use_double_quant,
            bnb_4bit_quant_type=self.bnb_4bit_quant_type,
        )

    def inference(self, prompt: Annotated[str, "Prompt text"] = "Write a sort algorithm.") -> Annotated[
        str, "Generated response"]:
        """
        Generate a response from the model based on the input prompt.

        Parameters
        ----------
        prompt : str, optional
            The input prompt to generate a response for.
            Defaults to "Write a sort algorithm."

        Returns
        -------
        str
            Generated text output.

        Raises
        ------
        ValueError
            If model or tokenizer is not loaded.

        Examples
        --------
        >>> from src.model.core import Assistant
        >>> from src.utils.type.schema import RootConfig
        >>> test_config_path = "src/config/config.yaml"
        >>> test_config_load = OmegaConf.load(config_path)
        >>> test_conf_dict = OmegaConf.to_container(config_load, resolve=True)
        >>> config = RootConfig(**conf_dict)
        >>> assistant = Assistant(config)
        >>> response = assistant.inference("Explain recursion.")
        >>> print(response)
        User: Explain recursion.
        Assistant: Recursion is...
        """
        if not isinstance(prompt, str):
            raise TypeError("Expected str for parameter prompt")

        if self.model is None or self.tokenizer is None:
            raise ValueError("Model/tokenizer not loaded. Call load() first.")

        text = f"User: {prompt}\nAssistant:"
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(**inputs, max_new_tokens=self.max_new_tokens)
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=self.skip_special_tokens)
        return result


class Student:
    """
    Student class for loading, saving, and performing inference with a
    distilled language model.

    This class manages a transformer-based student model, including loading
    from a local directory or Hugging Face Hub, setting up the tokenizer,
    saving model artifacts, and running inference.

    Parameters
    ----------
    config : RootConfig
        Configuration object containing student model setup and paths.

    Attributes
    ----------
    logger : logging.Logger
        Logger instance for logging messages.
    config : RootConfig
        Configuration data including student model paths.
    device : torch.device
        Device on which the model is loaded (CPU/GPU).
    model : AutoModelForCausalLM
        Loaded language model.
    tokenizer : AutoTokenizer
        Tokenizer corresponding to the language model.

    Examples
    --------
    >>> from src.model.core import Assistant
    >>> from src.utils.type.schema import RootConfig
    >>> test_config_path = "src/config/config.yaml"
    >>> test_config_load = OmegaConf.load(config_path)
    >>> test_conf_dict = OmegaConf.to_container(config_load, resolve=True)
    >>> config = RootConfig(**conf_dict)
    >>> test_student = Student(config)
    """

    def __init__(self, config: Annotated[RootConfig, "Configuration object"]):
        self.logger = Logger(name="Student").get()
        self.config = config

        device_str = (
            "cuda" if torch.cuda.is_available() and config.device.type == "auto"
            else config.device.type
        )
        self.device = torch.device(device_str)
        self.logger.info("Using device: %s", self.device)

        stu_conf = config.student
        local_path = stu_conf.save_path

        if os.path.isdir(local_path) and os.listdir(local_path):
            self.logger.info("Loading distilled student from local path: %s", local_path)
            self.model = AutoModelForCausalLM.from_pretrained(local_path).to(self.device)
            self.tokenizer = AutoTokenizer.from_pretrained(local_path, use_fast=False)
        else:
            fallback_repo = "bunyaminergen/Qwen2.5-Coder-1.5B-Instruct-SFT-Distilled"
            self.logger.info("Local model not found, loading fallback from HF Hub: %s", fallback_repo)
            self.model = AutoModelForCausalLM.from_pretrained(
                fallback_repo,
                device_map="auto",
                torch_dtype="auto",
            ).to(self.device)
            self.tokenizer = AutoTokenizer.from_pretrained(fallback_repo, use_fast=False)

        self._prepare_tokenizer()
        self.model.eval()

    def _prepare_tokenizer(self) -> None:
        """
        Configure tokenizer padding settings.

        Ensures that the tokenizer uses left-side padding and sets the pad
        token ID if not already defined.

        Returns
        -------
        None
        """
        self.tokenizer.padding_side = "left"
        if self.tokenizer.pad_token_id is None:
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

    def save(self) -> None:
        """
        Save the student model and tokenizer to the configured path.

        Returns
        -------
        None

        Examples
        --------
        >>> student.save()
        """
        out_dir = self.config.student.save_path
        os.makedirs(out_dir, exist_ok=True)
        self.logger.info("Saving student → %s", out_dir)
        self.model.save_pretrained(out_dir)
        self.tokenizer.save_pretrained(out_dir)

    def load(self) -> None:
        """
        Load the student model and tokenizer from the configured path.

        Returns
        -------
        None

        Examples
        --------
        >>> student.load()
        """
        ld = self.config.student.save_path
        self.logger.info("Loading student from %s", ld)
        self.tokenizer = AutoTokenizer.from_pretrained(ld, use_fast=False)
        self._prepare_tokenizer()
        self.model = AutoModelForCausalLM.from_pretrained(ld).to(self.device)
        self.model.eval()

    def inference(
            self,
            prompt: Annotated[str, "Prompt text to feed into the model"],
            max_new_tokens: Annotated[int, "Maximum number of tokens to generate"] = 256
    ) -> Annotated[str, "Generated response from the model"]:
        """
        Perform inference using the loaded student model.

        Parameters
        ----------
        prompt : str
            Prompt text to generate a response for.
        max_new_tokens : int, optional
            Maximum number of new tokens to generate. Default is 256.

        Returns
        -------
        str
            Generated text output from the model.

        Raises
        ------
        RuntimeError
            If the model is not loaded before inference.

        Examples
        --------
        >>> response = student.inference("How do you define gravity?")
        >>> print(response)
        System: ...
        User: How do you define gravity?
        Assistant: Gravity is the force...
        """
        if not isinstance(prompt, str):
            raise TypeError("Expected str for parameter prompt")
        if not isinstance(max_new_tokens, int):
            raise TypeError("Expected int for parameter max_new_tokens")

        if self.model is None:
            raise RuntimeError(
                "Student model not loaded. Call load() first or ensure a "
                "distilled model exists locally."
            )

        cfg = OmegaConf.load("src/config/prompt.yaml")
        system_prompt = PromptConfig(**OmegaConf.to_container(cfg, resolve=True)).Assistant.system
        text = f"System: {system_prompt}\nUser: {prompt}\nAssistant:"
        inputs = self.tokenizer(text, return_tensors="pt").to(self.device)

        with torch.no_grad():
            out = self.model.generate(**inputs, max_new_tokens=max_new_tokens)

        result = self.tokenizer.decode(out[0], skip_special_tokens=True)
        self.logger.info("Inference done.")
        print(result)
        return result


if __name__ == "__main__":
    config_path = "src/config/config.yaml"
    config_load = OmegaConf.load(config_path)
    conf_dict = OmegaConf.to_container(config_load, resolve=True)
    config_test = RootConfig(**conf_dict)

    student = Student(config_test)
    student.inference("Write a bubble‑sort implementation in Python.")
