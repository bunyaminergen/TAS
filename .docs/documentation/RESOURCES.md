# Resources

---

## Papers

- [Knowledge Distillation of Black-Box Large Language Models](https://arxiv.org/html/2401.07013v2)
- [Improved Knowledge Distillation via Teacher Assistant | Proceedings of the AAAI Conference on Artificial Intelligence](https://ojs.aaai.org/index.php/AAAI/article/view/5963)
- [\[2305.15781\] VanillaKD: Revisit the Power of Vanilla Knowledge Distillation from Small Scale to Large Scale](https://arxiv.org/abs/2305.15781)
- [stories260K/readme.md · karpathy/tinyllamas at main](https://huggingface.co/karpathy/tinyllamas/blob/main/stories260K/readme.md)
- [\[2401.02385\] TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385)
- [MiniLLM: Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2306.08543)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://ar5iv.labs.arxiv.org/html/2402.17764v1)
- [\[2310.11453\] BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453)
- [\[2402.17764\] The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://ar5iv.labs.arxiv.org/html/2402.17764)
- [Online Knowledge Distillation via Collaborative Learning](https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Online_Knowledge_Distillation_via_Collaborative_Learning_CVPR_2020_paper.pdf)
- [\[2209.04996\] Switchable Online Knowledge Distillation](https://arxiv.org/abs/2209.04996)
- [Online Knowledge Distillation with Diverse Peers | Proceedings of the AAAI Conference on Artificial Intelligence](https://ojs.aaai.org/index.php/AAAI/article/view/5746)
- [Online Knowledge Distillation: Advancing LLMs like Gemma 2 through Dynamic Learning | by Rishiraj Acharya | Google Developer Experts | Medium](https://medium.com/google-developer-experts/online-knowledge-distillation-advancing-llms-like-gemma-2-through-dynamic-learning-e61c39280693)
- [\[2207.11518\] Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition](https://arxiv.org/abs/2207.11518)
- [\[2312.05503\] Aligner: One Global Token is Worth Millions of Parameters When Aligning Large Language Models](https://arxiv.org/abs/2312.05503)
- [\[2302.14401\] GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation](https://arxiv.org/abs/2302.14401)
- [Introducing LMUnit: Natural language unit testing for LLM evaluation - Contextual AI](https://contextual.ai/lmunit/)
- [\[2412.13091\] LMUnit: Fine-grained Evaluation with Natural Language Unit Tests](https://arxiv.org/abs/2412.13091)
- [Paper page - DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://huggingface.co/papers/2402.03300)
- [2402.03300](https://arxiv.org/pdf/2402.03300)
- [The Chain of Self-Taught Knowledge Distillation Combining Output and Features | IEEE Conference Publication | IEEE Xplore](https://ieeexplore.ieee.org/document/9602623)
- [\[2403.03348\] Learning to Maximize Mutual Information for Chain-of-Thought Distillation](https://arxiv.org/abs/2403.03348)
- [\[2410.02952\] Visual Editing with LLM-based Tool Chaining: An Efficient Distillation Approach for Real-Time Applications](https://arxiv.org/abs/2410.02952)
- [\[2305.07759\] TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759)
- [\[2404.14827\] Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation](https://arxiv.org/abs/2404.14827)
- [Paper page - Knowledge Distillation via Token-level Relationship Graph](https://huggingface.co/papers/2306.12442)
- [Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion](https://www.isca-archive.org/interspeech_2019/sun19c_interspeech.pdf)
- [Knowledge Distillation via Token-Level Relationship Graph Based on the Big Data Technologies - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S2214579624000145)
- [\[2503.02832\] AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation](https://arxiv.org/abs/2503.02832)
- [\[2110.03742\] Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference](https://arxiv.org/abs/2110.03742)
- [Multi-Granularity Structural Knowledge Distillation for Language Model Compression - ACL Anthology](https://aclanthology.org/2022.acl-long.71/)
- [\[2012.00573\] Multi-level Knowledge Distillation via Knowledge Alignment and Correlation](https://arxiv.org/abs/2012.00573)
- [Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't](https://huggingface.co/papers/2503.16219)
- [Datasets for Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2402.18041)
- [\[2103.04062\] Adaptive Multi-Teacher Multi-level Knowledge Distillation](https://arxiv.org/abs/2103.04062)
- [\[1902.03393\] Improved Knowledge Distillation via Teacher Assistant](https://arxiv.org/abs/1902.03393)
- [\[2406.05322\] Teaching-Assistant-in-the-Loop: Improving Knowledge Distillation from Imperfect Teacher Models in Low-Budget Scenarios](https://arxiv.org/abs/2406.05322)

---

## Github

- [Zjh-819/LLMDataHub](https://github.com/Zjh-819/LLMDataHub)
- [lmmlzn/Awesome-LLMs-Datasets](https://github.com/lmmlzn/Awesome-LLMs-Datasets)
- [rasbt/LLMs-from-scratch: Implement LLM from scratch](https://github.com/rasbt/LLMs-from-scratch)
- [openai/tiktoken: tiktoken is a fast BPE tokeniser for use with OpenAI's models.](https://github.com/openai/tiktoken)
- [11868 Presentation](https://llmsystem.github.io/llmsystem2024spring/assets/files/Group2-Presentation-cf8028bc58193a5e6e6d7b05709ef1a9.pdf)
- [TinyBERT](https://phanxuanphucnd.github.io/language-modeling/TinyBERT)
- [microsoft/BitNet: Official inference framework for 1-bit LLMs](https://github.com/microsoft/BitNet)
- [trl/examples/scripts/sft_vlm.py at main · huggingface/trl](https://github.com/huggingface/trl/blob/main/examples/scripts/sft_vlm.py)
- [imirzadeh/Teacher-Assistant-Knowledge-Distillation: Using Teacher Assistants to Improve Knowledge Distillation: https://arxiv.org/pdf/1902.03393.pdf](https://github.com/imirzadeh/Teacher-Assistant-Knowledge-Distillation)

---

## Hugging Face

- [stories260K/readme.md · karpathy/tinyllamas at main](https://huggingface.co/karpathy/tinyllamas/blob/main/stories260K/readme.md)
- [Fine-tuning LLMs to 1.58bit: extreme quantization made easy](https://huggingface.co/blog/1_58_llm_extreme_quantization)
- [GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer)
- [ibm-granite/granite-3b-code-instruct-2k · Hugging Face](https://huggingface.co/ibm-granite/granite-3b-code-instruct-2k)
- [stabilityai/stable-code-instruct-3b · Hugging Face](https://huggingface.co/stabilityai/stable-code-instruct-3b)
- [replit/replit-code-v1-3b · Hugging Face](https://huggingface.co/replit/replit-code-v1-3b)
- [qualcomm/IBM-Granite-3B-Code-Instruct · Hugging Face](https://huggingface.co/qualcomm/IBM-Granite-3B-Code-Instruct)
- [Phi 4 Mini - a Hugging Face Space by microsoft](https://huggingface.co/spaces/microsoft/phi-4-mini)
- [microsoft/Phi-3-mini-128k-instruct · Hugging Face](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)
- [microsoft/Magma-8B · Hugging Face](https://huggingface.co/microsoft/Magma-8B)
- [microsoft/Phi-4-mini-instruct · Hugging Face](https://huggingface.co/microsoft/Phi-4-mini-instruct)
- [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)

---

## Book

- [Build a Large Language Model (From Scratch)](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167)

---

## Dataset

- [stingning/ultrachat · Datasets at Hugging Face](https://huggingface.co/datasets/stingning/ultrachat)
- [nreimers/reddit_question_best_answers](https://huggingface.co/datasets/nreimers/reddit_question_best_answers)
- [HuggingFaceH4/ultrachat_200k · Datasets at Hugging Face](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)
- [nikhiljatiwal/Llama-3.2-Python-Alpaca-143k · Datasets at Hugging Face](https://huggingface.co/datasets/nikhiljatiwal/Llama-3.2-Python-Alpaca-143k)

---

## Web

- [What are Small Language Models (SLM)? | IBM](https://www.ibm.com/think/topics/small-language-models)
- [Small Language Models: The next frontier in enterprise AI](https://www.techcircle.in/2024/07/26/small-language-models-the-next-frontier-in-enterprise-ai)
- [Small is the new big: the rise of small language models - Capgemini Belgium](https://www.capgemini.com/be-en/insights/expert-perspectives/small-is-the-new-big-the-rise-of-small-language-models/)
- [Microsoft Announces Small Language Model Phi-2 - InfoQ](https://www.infoq.com/news/2023/12/microsoft-llm-phi/)
- [Phi Open Models - Small Language Models | Microsoft Azure](https://azure.microsoft.com/en-us/products/phi)
- [AI: What are SLMs and are they good for businesses? | World Economic Forum](https://www.weforum.org/stories/2025/01/ai-small-language-models/)
- [Are there any LLMs with less than 1m parameters? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1iv72vu/are_there_any_llms_with_less_than_1m_parameters/)
- [Apple releases eight OpenELM AI small language models for on-device use](https://www.computing.co.uk/news/4202434/apple-releases-openelm-ai-small-language-models-device)
- [LLM distillation demystified: a complete guide | Snorkel AI](https://snorkel.ai/blog/llm-distillation-demystified-a-complete-guide/)
- [Distillation: Turning Smaller Models into High-Performance, Cost-Effective Solutions | Microsoft Community Hub](https://techcommunity.microsoft.com/blog/aiplatformblog/distillation-turning-smaller-models-into-high-performance-cost-effective-solutio/4355029)
- [A Practitioner's Guide on Inferencing over 1-bit LLMs using bitnet.cpp](https://adasci.org/a-practitioners-guide-on-inferencing-over-1-bit-llms-using-bitnet-cpp/)
- [LoftQ: Reimagining LLM fine-tuning with smarter initialization - Microsoft Research](https://www.microsoft.com/en-us/research/blog/loftq-reimagining-llm-fine-tuning-with-smarter-initialization/)
- [1-bit Adam: Up to 5x less communication volume and up to 3.4x faster training - DeepSpeed](https://www.deepspeed.ai/tutorials/onebit-adam/)
- [1-bit LLMs by Microsoft | Implementing GPT in 60 Lines – Up As Pro – Code](https://upaspro.com/1-bit-llms-by-microsoft-implementing-gpt-in-60-lines/)
- [How Microsoft's next-gen BitNet architecture is turbocharging LLM efficiency | VentureBeat](https://venturebeat.com/ai/how-microsofts-next-gen-bitnet-architecture-is-turbocharging-llm-efficiency/)
- [What is bitnet.cpp? Framework for Inferring 1-bit LLM by Microsoft in C/C++ Language](https://japanesetarheel.com/blog/bitnet-cpp-in-c-language/)
- [Contextual AI’s new AI model crushes GPT-4o in accuracy — here’s why it matters | VentureBeat](https://venturebeat.com/ai/contextual-ais-new-ai-model-crushes-gpt-4o-in-accuracy-heres-why-it-matters/)
- [Introducing the most grounded language model in the world - Contextual AI](https://contextual.ai/blog/introducing-grounded-language-model/)
- [AWS | Community | Deep dive into Group Relative Policy Optimization (GRPO)](https://community.aws/content/2rJrpj6m2eh591fjMcRZ3ushpB7/deep-dive-into-group-relative-policy-optimization-grpo)
- [Knowledge Distillation: Principles, Algorithms, Applications](https://neptune.ai/blog/knowledge-distillation)
- [To Make Language Models Work Better, Researchers Sidestep Language | Quanta Magazine](https://www.quantamagazine.org/to-make-language-models-work-better-researchers-sidestep-language-20250414/)

--- 

## Medium

- [The Rise of Small Language Models— Efficient & Customizable | by Bijit Ghosh | Medium](https://medium.com/@bijit211987/the-rise-of-small-language-models-efficient-customizable-cb48ddee2aad)
- [The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO) | by Sahin Ahmed, Data Scientist | Jan, 2025 | Medium](https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba)
- [What is GRPO? The RL algorithm used to train DeepSeek | by Mehul Gupta | Data Science in your pocket | Jan, 2025 | Medium](https://medium.com/data-science-in-your-pocket/what-is-grpo-the-rl-algorithm-used-to-train-deepseek-12acc19798d3)
- [DeepSeek R1 Explained: Chain of Thought, Reinforcement Learning, and Model Distillation | by Tahir | Jan, 2025 | Medium](https://medium.com/@tahirbalarabe2/deepseek-r1-explained-chain-of-thought-reinforcement-learning-and-model-distillation-0eb165d928c9)

---

## Youtube

- [TinyStories: 1 Million Parameter LLM Model That Produces Coherent Text! - YouTube](https://www.youtube.com/watch?v=ZvfOWVRSY0w)
- [Can I Turn A 158 Million Parameter LLM Into A GPT01 Level Model? - YouTube](https://www.youtube.com/watch?v=Kb2TA0_QqWY)
- [Does LLM Size Matter? How Many Billions of Parameters do you REALLY Need? - YouTube](https://www.youtube.com/watch?v=kgSMRmW2frA)
- [Group Relative Policy Optimization (GRPO) - Formula and Code - YouTube](https://www.youtube.com/watch?v=Yi1UCrAsf4o)
- [KL Divergence in DeepSeek R1 | Implementation Walk-through - YouTube](https://www.youtube.com/watch?v=iHf6mMiiNOw)

---

## Notebook

- [TinyStories.ipynb - Colab](https://colab.research.google.com/drive/1su0e3BMoC-cj97kub0lNcc4FBvgYTy1l/)